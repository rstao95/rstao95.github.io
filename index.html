<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Renshuai Tao (陶仁帅)</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="https://rstao95.github.io/Imgs/buaa_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Renshuai Tao (陶仁帅)</name></p>
                <p align="justify">I am a PhD student (2019.09-) in the State Key Laboratory of Software Development Environment (SKLSDE)
                    at <a href="https://www.buaa.edu.cn/">Beihang University</a>,
                    supervised by Prof. <a href="http://sites.nlsde.buaa.edu.cn/~liwei/">Wei Li</a>
                    and Prof. <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>.
                    I obtained my BSc degree in Computer Science and Engineering from <a href="https://www.buaa.edu.cn/">Beihang University</a> in 2017.
                    <br><br>
                    <strong>Email:</strong> rstao [AT] buaa.edu.cn
                <br>

                </p><p align="center">
                    <a href="https://github.com/rstao95"> Github </a> / 
                    <a href="https://scholar.google.com/citations?user=IGDmVHsAAAAJ&hl=zh-CN">Google Scholar</a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./Imgs/photo4.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
            <p align="justify">I'm interested in object detection in computer vision. 
               And my research goal is to establish evaluation benchmarks for some very important areas that have not been studied recently. I hope these benchmarks can contribute to the community and encourage more researchers to continue to work in this field.
            </p>
               My research focus is mainly on:
                <ul>
                    <li>
                       De-occlusion detection
                    </li>
                    <li>
                        Small object detection
                    </li>
                    <li>
                        Domain adaptation
                    </li>
                    <li>
                        Few-shot detection
                    </li>
                    <li>
                        Unsupervised Learning
                    </li>
                </ul>
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading> 
            <p> <strong>[2022.03.02]</strong> Two papers including one first-authored are accepted by <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.
            <p> <strong>[2021.07.23]</strong> One first-authored <a href="https://rstao95.github.io/Slides/ICCV_2021.pdf ">paper</a> for object detection is accepted by <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
            <p> <strong>[2021.08.21]</strong> One corresponding-authored paper</a> for scene text recognition is accepted by <a href="https://www.journals.elsevier.com/journal-of-visual-communication-and-image-representation">Elsevier JVCI</a>.
            <p> <strong>[2021.05.05]</strong> I am selected in Doctoral Consortium <a href="https://ijcai-21.org/program-dc/">IJCAI 2021</a> (<font color="red">7 people from Mainland China</font>).</p>
            <p> <strong>[2021.03.01]</strong> One co-authored <font color="red"><strong>oral</strong></font> <a href="https://arxiv.org/pdf/2103.01049.pdf">paper</a> for data-free quantization is accepted by <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
            <p> <strong>[2021.01.13]</strong> One co-first-authored <a href="https://arxiv.org/pdf/2103.05985.pdf">paper</a> for Unsupervised few-shot learning is accepted by <a href="https://2021.ieeeicme.org/">ICME 2021</a>.</p>
            <p> <strong>[2020.07.26]</strong> One co-first-authored <font color="red"><strong>oral</strong></font> <a href="https://dl.acm.org/doi/10.1145/3394171.3413828">paper</a> for De-occlusion X-ray Detection is accepted by <a href="https://2020.acmmm.org/">ACM MM 2020</a>.</p>
          </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td>
          <heading>Selected Papers<a name="publications"></a> | 
          <a href="https://rstao95.github.io/publications.html"><font color="grey"><heading>All Publications</heading></font></a>
        </heading>
        </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>

                <tr><td width="20%"><img src="./Imgs/CVPR_2022_1.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://arxiv.org/abs/2108.09917">
                <papertitle>Exploring Endogenous Shift for Cross-domain Detection: A Large-scale Benchmark and Perturbation Suppression Network.</papertitle></a>
                [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_Exploring_Endogenous_Shift_for_Cross-Domain_Detection_A_Large-Scale_Benchmark_and_CVPR_2022_paper.pdf">PDF</a>]
                <br><strong>Renshuai Tao</strong>, Hainan Li, Tianbo Wang, Yanlu Wei, Yifu Ding, Bowei Jin, Hongping Zhi, Xianglong Liu, Aishan Liu. 
                <br>
                <em>IEEE/CVF International Conference on Computer Vision and Pattern Recognition(CVPR), CCF-A</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2108.09917">arXiv</a> / 
                <a href="https://github.com/HiXray-author/HiXray"><font color="red">Code</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=rstao95&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">We first put forward a novel and important type of domain shift in cross-domain detection, the endogenous shift, which may cause severe performance drop but has been rarely studied.</p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/ICCV_2021.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2108.09917">
                <papertitle>Towards Real-world X-ray Security Inspection: A High-Quality Benchmark And Lateral Inhibition Module For Prohibited Items Detection.</papertitle></a>
                [<a href="https://arxiv.org/abs/2108.09917">PDF</a>]
                <br><strong>Renshuai Tao</strong>, Yanlu Wei, Xiangjian Jiang, Hainan Li, Haotong Qin, Jiakai Wang, Yuqing Ma, Libo Zhang, Xianglong Liu. 
                <br>
                <em>IEEE International Conference on Computer Vision (ICCV), CCF-A</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2108.09917">arXiv</a> / 
                <a href="https://github.com/HiXray-author/HiXray"><font color="red">Code</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=rstao95&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">We present a High-quality X-ray (HiXray) security inspection image dataset and the Lateral Inhibition Module (LIM).</p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/CYB.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2103.00809">
                <papertitle>Over-sampling De-occlusion Attention Network for Prohibited Items Detection in Noisy X-ray Images</papertitle></a>
                [<a href="https://arxiv.org/pdf/2103.00809.pdf">PDF</a>]
                <br><strong>Renshuai Tao</strong>, Yanlu Wei, Hainan Li, Aishan Liu, Yifu Ding, Haotong Qin, Xianglong Liu.
                <br>
                <em>ArXiv</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2103.00809">arXiv</a>/ 
                <a href="https://github.com/OPIXray-author/OPIXray"><font color="red">Code</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=rstao95&repo=IR-Net&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">To better improve occluded X-ray object detection, we further propose an over-sampling de-occlusion attention network (DOAM-O), which consists of a novel de-occlusion attention module and a new over-sampling training strategy.</p>
            </td>
        </tr>


        <tr><td width="20%"><img src="./Imgs/ACMMM2020.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://dl.acm.org/doi/10.1145/3394171.3413828">
                <papertitle>Occluded Prohibited Items Detection: An X-ray Security Inspection Benchmark and De-occlusion Attention Module.</papertitle></a>
                [<a href="https://arxiv.org/pdf/2004.08656.pdf">PDF</a>]
                <br>Yanlu Wei*, <strong>Renshuai Tao*</strong>, Zhangjie Wu, Yuqing Ma, Libo Zhang, Xianglong Liu. (* indicates equal contribution)
                <br>
                <em>ACM Multimedia (ACM MM), CCF-A</em>,<font color="red"><strong style="font-size:13px">Oral</strong></font>, 2020
                <br>
                <a href="https://arxiv.org/abs/2004.08656">arXiv</a> / 
                <a href="https://github.com/OPIXray-author/OPIXray"><font color="red">Code</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=rstao95&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">We contribute the first high-quality object detection dataset for security inspection, named Occluded Prohibited Items X-ray (OPIXray) image benchmark. </p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/ICME.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2103.05985">
                <papertitle>Multi-Pretext Attention Network for Few-shot Learning with Self-supervision.</papertitle></a>
                [<a href="https://arxiv.org/pdf/2103.05985.pdf">PDF</a>]
                <br>Hainan Li*, <strong>Renshuai Tao*</strong>, Jun Li, Haotong Qin, Yifu Ding, Shuo Wang, Xianglong Liu. (* indicates equal contribution)
                <br>
                <em>IEEE International Conference on Multimedia and Expo (ICME), CCF-B</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2103.05985">arXiv</a> / 
                <a href="https://github.com/MAN-author/MAN"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=rstao95&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">In this work, we propose a Graph-driven Clustering (GC), a novel augmentation-free method for self-supervised learning, which does not rely on any auxiliary sample and utilizes the endogenous correlation information among input samples. </p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/JVCI.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2103.05985">
                <papertitle>Sequential Alignment Attention Model for Scene Text Recognition.</papertitle></a>
                [<a href="https://arxiv.org/pdf/2103.05985.pdf">PDF</a>]
                <br>Yan Wu, Jiaxin Fan, <strong>Renshuai Tao*</strong>, Jiakai Wang, Haotong Qin, Aishan Liu, Xianglong Liu. (* indicates the corresponding author)
                <br>
                <em>ELSEVIER Journal of Visual Communication and Image Representation (JVCI) (SCI, Q2)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2103.05985">arXiv</a> / 
                <a href="https://github.com/MAN-author/MAN"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=rstao95&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">In this paper, we proposes a sequential alignment attention model to enhance the alignment between input images and output character sequences.</p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/dsg.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2103.01049">
                <papertitle>Diversifying Sample Generation for Accurate Data-Free Quantization</papertitle></a>
                [<a href="https://arxiv.org/pdf/2103.01049.pdf">PDF</a>]
                <br>Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, <strong>Renshuai Tao</strong>, Yuhang Li, Fengwei Yu, Xianglong Liu.
                <br>
                <em style="font-size:13px">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CCF-A</em> <font color="red"><strong style="font-size:13px">Oral</strong></font>, <font style="font-size:13px">2021</font>
                <br>
                <a href="https://arxiv.org/abs/2103.01049">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/0qIBM4wJTc12WzghV1V_gQ"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/WftpvEWa_BAyljyyRSIEVQ"><font color="red">商汤学术) </font></a>
                <br>
                <p align="justify" style="font-size:13px">We proposed Diverse Sample Generation (DSG) scheme to mitigate the adverse effects caused by homogenization in data-free quantization, 
                    which obtained significant improvements over various networks and quantization methods.</p>
            </td>
        </tr>



        <tr><td width="20%"><img src="./Imgs/TMM_HASH.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://ieeexplore.ieee.org/document/9019840">
	             <papertitle>Fast Nearest Subspace Search via Random Angular Hashing.</papertitle></a>
                 <br>Yi Xu, Xianglong Liu, Binshuai Wang, <strong>Renshuai Tao</strong>, Ke Xia, Xinbin Cao.
                 <br>
                 <em>IEEE Transactions on Multimedia (TMM), SCI, Q1</em>, 2020
                 <br>
                 </p><p align="justify" style="font-size:13px">In this paper, we propose random angular hashing, a new and efficient type of locality-sensitive hashing, for linear subspaces of arbitrary dimension. </p>
                <p></p>
            </td>
        </tr>
        </tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Honors</heading>
            <p> <strong>[2021.09]</strong> &nbsp;&nbsp; National Scholarship (<strong>Top 2%</strong>).</p>
            <p> <strong>[2021.01]</strong> &nbsp;&nbsp; Outstanding Postgraduate of Beihang University (<strong>Top 10%</strong>).</p>

             <p> <strong>[2020.11]</strong> &nbsp;&nbsp; Merit Student of Beihang University.</p>

             <p> <strong>[2020.11]</strong> &nbsp;&nbsp; the First Prize of Excellent Postgraduate Scholarship of Beihang University (<strong>Top 20%</strong>).</p>

             <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Beihang Ph.D. Scholarship for Outstanding Freshmen (<strong>Top 10%</strong>).</p>

            </td>
            </tr></tbody>
    </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Talks & Academic Services</heading>
           <p style="font-size:13px"> <strong>[2021.08]</strong> I am invited to talk about X-ray Object Detection at preparatory meeting of ICCV 2021 in China, hosted by China Society of Image and Graphics. [<a href="https://event.baai.ac.cn/event/162">Video</a>] [<a href="https://rstao95.github.io/Slides/ICCV2021_China.pdf">Slides</a>] </p>
           <p style="font-size:13px"> <strong>[2020.10]</strong> I am invited to talk about Object Detection in Complex X-ray Scenario at ACMMM 2021. [<a href="https://dl.acm.org/doi/10.1145/3394171.3413828">Video</a>] </p>
            <p style="font-size:13px"> <strong>[2020-]</strong> I regularly review papers for top-tier conferences (ACM MM, AAAI, ...) and journals (Pattern Recognition, Knowledge-based System, ...) in machine learning and computer vision.</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Teaching</heading>
            <p> <strong>[Fall 2020]</strong> &nbsp;&nbsp; Teaching Assistant in Machine Learning (Beihang University).</p>
            <p> <strong>[Fall 2019]</strong> &nbsp;&nbsp; Teaching Assistant in Machine Learning (Beihang University).</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">I am a person who enjoys both academic and life. In my spare time, I like to have dinner with my friends, travel in the suburbs and take photos with my camera. Warmth and purity, love and freedom.
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 9 -->
    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=pv9ptbYYCifwCzvjv_lWOT6Ih6l9g4QJSPj9vqWhbh8&cl=ffffff&w=a"></script>
    </p></td>
    </tr>
    </tbody>
    </table>